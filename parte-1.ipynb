{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "collapsed": true
            },
            "source": "# MARATONA BEHIND THE CODE 2020\n\n## DESAFIO 2: PARTE 1"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Introdu\u00e7\u00e3o"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Em projetos de ci\u00eancia de dados visando a constru\u00e7\u00e3o de modelos de *machine learning*, ou aprendizado estat\u00edstico, \u00e9 muito incomum que os dados iniciais estejam j\u00e1 no formato ideal para a constru\u00e7\u00e3o de modelos. S\u00e3o necess\u00e1rios v\u00e1rios passos intermedi\u00e1rios de pr\u00e9-processamento de dados, como por exemplo a codifica\u00e7\u00e3o de vari\u00e1veis categ\u00f3ricas, normaliza\u00e7\u00e3o de vari\u00e1veis num\u00e9ricas, tratamento de dados faltantes, etc. A biblioteca **scikit-learn** -- uma das mais populares bibliotecas de c\u00f3digo-aberto para *machine learning* no mundo -- possui diversas fun\u00e7\u00f5es j\u00e1 integradas para a realiza\u00e7\u00e3o das transforma\u00e7\u00f5es de dados mais utilizadas. Entretanto, em um fluxo comum de um modelo de aprendizado de m\u00e1quina, \u00e9 necess\u00e1ria a aplica\u00e7\u00e3o dessas transforma\u00e7\u00f5es pelo menos duas vezes: a primeira vez para \"treinar\" o modelo, e depois novamente quando novos dados forem enviados como entrada para serem classificados por este modelo. \n\nPara facilitar o trabalho com esse tipo de fluxo, o scikit-learn possui tamb\u00e9m uma ferramenta chamada **Pipeline**, que nada mais \u00e9 do que uma lista ordenada de transforma\u00e7\u00f5es que devem ser aplicadas nos dados. Para auxiliar no desenvolvimento e no gerenciamento de todo o ciclo-de-vida dessas aplica\u00e7\u00f5es, alem do uso de Pipelines, as equipes de cientistas de dados podem utilizar em conjunto o **Watson Machine Learning**, que possui dezenas de ferramentas para treinar, gerenciar, hospedar e avaliar modelos baseados em aprendizado de m\u00e1quina. Al\u00e9m disso, o Watson Machine Learning \u00e9 capaz de encapsular pipelines e modelos em uma API pronta para uso e integra\u00e7\u00e3o com outras aplica\u00e7\u00f5es.\n\nDurante o desafio 2, voc\u00ea participante ir\u00e1 aprender a construir uma **Pipeline** para um modelo de classifica\u00e7\u00e3o e hosped\u00e1-lo como uma API com o aux\u00edlio do Watson Machine Learning. Uma vez hospedado, voc\u00ea poder\u00e1 integrar o modelo criado com outras aplica\u00e7\u00f5es, como assistentes virtuais e muito mais. Neste notebook, ser\u00e1 apresentado um exemplo funcional de cria\u00e7\u00e3o de um modelo e de uma pipeline no scikit-learn (que voc\u00ea poder\u00e1 utilizar como template para a sua solu\u00e7\u00e3o!)."
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## ** ATEN\u00c7\u00c3O **\n\nEste notebook serve apenas um prop\u00f3sito educativo, voc\u00ea pode alterar o c\u00f3digo como quiser e nada aqui ser\u00e1 avaliado/pontuado.\n\nA recomenda\u00e7\u00e3o \u00e9 que voc\u00ea experimente e teste diferentes algoritmos aqui antes de passar para a *parte-2*, onde ser\u00e1 realizado o deploy do seu modelo no **Watson Machine Learning** :)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Trabalhando com Pipelines do scikit-learn"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Requirement already up-to-date: scikit-learn==0.20.3 in /opt/conda/envs/Python36/lib/python3.6/site-packages (0.20.3)\nRequirement already satisfied, skipping upgrade: numpy>=1.8.2 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from scikit-learn==0.20.3) (1.15.4)\nRequirement already satisfied, skipping upgrade: scipy>=0.13.3 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from scikit-learn==0.20.3) (1.2.0)\nCollecting xgboost==0.71\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4b/c4/57e246bc99e45c048f9805f2773e7369f0d30896d19fa089fa1794c7b246/xgboost-0.71.tar.gz (494kB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 501kB 10.7MB/s eta 0:00:01\n"
                }
            ],
            "source": "# Primeiro, realizamos a instala\u00e7\u00e3o do scikit-learn vers\u00e3o 0.20.3 e do xgboost vers\u00e3o 0.71 no Kernel deste notebook\n# ** CUIDADO AO TROCAR A VERS\u00c3O DAS BIBLIOTECAS -- VERS\u00d5ES DIFERENTES PODEM SER INCOMPAT\u00cdVEIS COM O WATSON STUDIO **\n# OBS: A instala\u00e7\u00e3o do xgboost leva um tempo consider\u00e1vel\n!pip install scikit-learn==0.20.3 --upgrade\n!pip install xgboost==0.71 --upgrade"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# Em seguida iremos importar diversas bibliotecas que ser\u00e3o utilizadas:\n\n# Pacote para trabalhar com JSON\nimport json\n\n# Pacote para realizar requisi\u00e7\u00f5es HTTP\nimport requests\n\n# Pacote para explora\u00e7\u00e3o e an\u00e1lise de dados\nimport pandas as pd\n\n# Pacote com m\u00e9todos num\u00e9ricos e representa\u00e7\u00f5es matriciais\nimport numpy as np\n\n# Pacote para constru\u00e7\u00e3o de modelo baseado na t\u00e9cnica Gradient Boosting\nimport xgboost as xgb\n\n# Pacotes do scikit-learn para pr\u00e9-processamento de dados\n# \"SimpleImputer\" \u00e9 uma transforma\u00e7\u00e3o para preencher valores faltantes em conjuntos de dados\nfrom sklearn.impute import SimpleImputer\n\n# Pacotes do scikit-learn para treinamento de modelos e constru\u00e7\u00e3o de pipelines\n# M\u00e9todo para separa\u00e7\u00e3o de conjunto de dados em amostras de treino e teste\nfrom sklearn.model_selection import train_test_split\n# M\u00e9todo para cria\u00e7\u00e3o de modelos baseados em \u00e1rvores de decis\u00e3o\nfrom sklearn.tree import DecisionTreeClassifier\n# Classe para a cria\u00e7\u00e3o de uma pipeline de machine-learning\nfrom sklearn.pipeline import Pipeline\n\n# Pacotes do scikit-learn para avalia\u00e7\u00e3o de modelos\n# M\u00e9todos para valida\u00e7\u00e3o cruzada do modelo criado\nfrom sklearn.model_selection import KFold, cross_validate"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Importando um .csv de seu projeto no IBM Cloud Pak for Data para o Kernel deste notebook"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Primeiro iremos importar o dataset fornecido para o desafio, que j\u00e1 est\u00e1 incluso neste projeto!\n\nVoc\u00ea pode realizar a importa\u00e7\u00e3o dos dados de um arquivo .csv diretamente para o Kernel do notebook como um DataFrame da biblioteca Pandas, muito utilizada para a manipula\u00e7\u00e3o de dados em Python.\n\nPara realizar a importa\u00e7\u00e3o, basta selecionar a pr\u00f3xima c\u00e9lula e seguir as instru\u00e7\u00f5es na imagem abaixo:\n\n![alt text](https://i.imgur.com/K1DwL9I.png \"importing-csv-as-df\")\n\nAp\u00f3s a sele\u00e7\u00e3o da op\u00e7\u00e3o **\"Insert to code\"**, a c\u00e9lula abaixo ser\u00e1 preenchida com o c\u00f3digo necess\u00e1rio para importa\u00e7\u00e3o e leitura dos dados no arquivo .csv como um DataFrame Pandas."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# The code was removed by Watson Studio for sharing."
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Temos 15 colunas presentes no dataset fornecido, sendo dezessete delas vari\u00e1veis caracter\u00edsticas (dados de entrada) e um delas uma vari\u00e1vel-alvo (que queremos que o nosso modelo seja capaz de prever). \n\nAs vari\u00e1veis caracter\u00edsticas s\u00e3o:\n\n    MATRICULA       - n\u00famero de matr\u00edcula do estudante\n    NOME            - nome completo do estudante\n    REPROVACOES_DE  - n\u00famero de reprova\u00e7\u00f5es na disciplina de ``Direito Empresarial``\n    REPROVACOES_EM  - n\u00famero de reprova\u00e7\u00f5es na disciplina de ``Empreendedorismo``\n    REPROVACOES_MF  - n\u00famero de reprova\u00e7\u00f5es na disciplina de ``Matem\u00e1tica Financeira``\n    REPROVACOES_GO  - n\u00famero de reprova\u00e7\u00f5es na disciplina de ``Gest\u00e3o Operacional``\n    NOTA_DE         - m\u00e9dia simples das notas do aluno na disciplina de ``Direito Empresarial`` (0-10)\n    NOTA_EM         - m\u00e9dia simples das notas do aluno na disciplina de ``Empreendedorismo`` (0-10)\n    NOTA_MF         - m\u00e9dia simples das notas do aluno na disciplina de ``Matem\u00e1tica Financeira`` (0-10)\n    NOTA_GO         - m\u00e9dia simples das notas do aluno na disciplina de ``Gest\u00e3o Operacional`` (0-10)\n    INGLES          - vari\u00e1vel bin\u00e1ria que indica se o estudante tem conhecimento em l\u00edngua inglesa (0 -> sim ou 1 -> n\u00e3o).\n    H_AULA_PRES     - horas de estudo presencial realizadas pelo estudante\n    TAREFAS_ONLINE  - n\u00famero de tarefas online entregues pelo estudante\n    FALTAS          - n\u00famero de faltas acumuladas do estudante (todas disciplinas)\n    \nA vari\u00e1vel-alvo \u00e9:\n\n    PERFIL               - uma *string* que indica uma de cinco possibilidades: \n        \"EXCELENTE\"      - Estudante n\u00e3o necessita de mentoria\n        \"MUITO BOM\"      - Estudante n\u00e3o necessita de mentoria\n        \"HUMANAS\"        - Estudante necessita de mentoria exclusivamente em mat\u00e9rias com conte\u00fado de ci\u00eancias humanas\n        \"EXATAS\"         - Estudante necessita de mentoria apenas em disciplinas com conte\u00fado de ci\u00eancias exatas\n        \"DIFICULDADE\"    - Estudante necessita de mentoria em duas ou mais disciplinas\n        \nCom um modelo capaz de classificar um estudante em uma dessas categorias, podemos automatizar parte da mentoria estudantil atrav\u00e9s de assistentes virtuais, que ser\u00e3o capazes de recomendar pr\u00e1ticas de estudo e conte\u00fado personalizado com base nas necessidades de cada aluno."
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Explorando os dados fornecidos\n\nPodemos continuar a explora\u00e7\u00e3o dos dados fornecidos com a fun\u00e7\u00e3o ``info()``:"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": "df.info()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "\u00c9 notado que existem vari\u00e1veis do tipo ``float64`` (n\u00fameros \"decimais\"), vari\u00e1veis do tipo ``int64`` (n\u00fameros inteiros) e do tipo ``object`` (nesse caso s\u00e3o *strings*, ou texto). \n\nComo a maioria dos algoritmos de aprendizado estat\u00edstico supervisionado s\u00f3 aceita valores num\u00e9ricos como entrada, \u00e9 necess\u00e1rio ent\u00e3o o pr\u00e9-processamento das vari\u00e1veis do tipo \"object\" antes de usar esse dataset como entrada para o treinamento de um modelo. Tamb\u00e9m \u00e9 notado que existem valores faltantes em v\u00e1rias colunas. Esses valores faltantes tamb\u00e9m devem ser tratados antes de serem constru\u00eddos modelos com esse conjunto de dados base."
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "A fun\u00e7\u00e3o ``describe()`` gera v\u00e1rias informa\u00e7\u00f5es sobre as vari\u00e1veis num\u00e9ricas que tamb\u00e9m podem ser \u00fateis:"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": "df.describe()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Visualiza\u00e7\u00f5es\n\nPara visualizar o dataset fornecido, podemos utilizar as bibliotecas ``matplotlib`` e ``seaborn``:"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": "fig, axes = plt.subplots(nrows=1, ncols=4, figsize=(28, 4))\n\nsns.countplot(ax=axes[0], x='REPROVACOES_DE', data=df)\nsns.countplot(ax=axes[1], x='REPROVACOES_EM', data=df)\nsns.countplot(ax=axes[2], x='REPROVACOES_MF', data=df)\nsns.countplot(ax=axes[3], x='REPROVACOES_GO', data=df)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "fig, axes = plt.subplots(nrows=1, ncols=4, figsize=(28, 4))\n\nsns.distplot(df['NOTA_DE'], ax=axes[0])\nsns.distplot(df['NOTA_EM'], ax=axes[1])\nsns.distplot(df['NOTA_MF'], ax=axes[2])\nsns.distplot(df['NOTA_GO'].dropna(), ax=axes[3])"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": "fig, axes = plt.subplots(nrows=1, ncols=4, figsize=(28, 4))\n\nsns.countplot(ax=axes[0], x='INGLES', data=df)\nsns.countplot(ax=axes[1], x='FALTAS', data=df)\nsns.countplot(ax=axes[2], x='H_AULA_PRES', data=df)\nsns.countplot(ax=axes[3], x='TAREFAS_ONLINE', data=df)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "fig = plt.plot()\nsns.countplot(x='PERFIL', data=df)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## ** ATEN\u00c7\u00c3O **\n\nVoc\u00ea pode notar pela figura acima que este dataset \u00e9 desbalanceado, isto \u00e9, a quantidade de amostras para cada classe que desejamos classificar \u00e9 bem discrepante. O participante \u00e9 livre para adicionar ou remover **LINHAS** no dataset fornecido, inclusive utilizar bibliotecas para balanceamento com ``imblearn``. Entretanto tome **muito cuidado**!!! Voc\u00ea n\u00e3o pode alterar os tipos dos dados e nem remover ou desordenar o dataset fornecido. Todas as opera\u00e7\u00f5es desse tipo dever\u00e3o ser feitas por meio de Transforms do scikit-learn :)\n\n<hr>"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Realizando o pr\u00e9-processamento dos dados"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Para o pr\u00e9-processamento dos dados ser\u00e3o apresentadas duas transforma\u00e7\u00f5es b\u00e1sicas neste notebook, demonstrando a constru\u00e7\u00e3o de uma Pipeline com um modelo funcional. Esta Pipeline funcional fornecida dever\u00e1 ser melhorada pelo participante para que o modelo final alcance a maior acur\u00e1cia poss\u00edvel, garantindo uma pontua\u00e7\u00e3o maior no desafio. Essa melhoria pode ser feita apenas no pr\u00e9-processamento dos dados, na escolha de um algoritmo para treinamento de modelo diferente, ou at\u00e9 mesmo na altera\u00e7\u00e3o do *framework* usado (entretanto s\u00f3 ser\u00e1 fornecido um exemplo pronto de integra\u00e7\u00e3o do Watson Machine Learning com o *scikit-learn*).\n\nA primeira transforma\u00e7\u00e3o (passo na nossa Pipeline) ser\u00e1 a exclus\u00e3o da coluna \"NOME\" do nosso dataset, que al\u00e9m de n\u00e3o ser uma vari\u00e1vel num\u00e9rica, tamb\u00e9m n\u00e3o \u00e9 uma vari\u00e1vel relacionada ao desempenho dos estudantes nas disciplinas. Existem fun\u00e7\u00f5es prontas no scikit-learn para a realiza\u00e7\u00e3o dessa transforma\u00e7\u00e3o, entretanto nosso exemplo ir\u00e1 demonstrar como criar uma transforma\u00e7\u00e3o personalizada do zero no scikit-learn. Se desejado, o participante poder\u00e1 utilizar esse exemplo para criar outras transforma\u00e7\u00f5es e adicion\u00e1-las \u00e0 Pipeline final :)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "#### Transforma\u00e7\u00e3o 1: excluindo colunas do dataset\n\nPara a cria\u00e7\u00e3o de uma transforma\u00e7\u00e3o de dados personalizada no scikit-learn, \u00e9 necess\u00e1ria basicamente a cria\u00e7\u00e3o de uma classe com os m\u00e9todos ``transform`` e ``fit``. No m\u00e9todo transform ser\u00e1 executada a l\u00f3gica da nossa transforma\u00e7\u00e3o.\n\nNa pr\u00f3xima c\u00e9lula \u00e9 apresentado o c\u00f3digo completo de uma transforma\u00e7\u00e3o ``DropColumns`` para a remo\u00e7\u00e3o de colunas de um DataFrame pandas."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "from sklearn.base import BaseEstimator, TransformerMixin\n\n\n# All sklearn Transforms must have the `transform` and `fit` methods\nclass DropColumns(BaseEstimator, TransformerMixin):\n    def __init__(self, columns):\n        self.columns = columns\n\n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X):\n        # Primeiro realizamos a c\u00f3pia do dataframe 'X' de entrada\n        data = X.copy()\n        # Retornamos um novo dataframe sem as colunas indesejadas\n        return data.drop(labels=self.columns, axis='columns')"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Para aplicar essa transforma\u00e7\u00e3o em um DataFrame pandas, basta instanciar um objeto *DropColumns* e chamar o m\u00e9todo transform()."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# Instanciando uma transforma\u00e7\u00e3o DropColumns\nrm_columns = DropColumns(\n    columns=[\"NOME\"]  # Essa transforma\u00e7\u00e3o recebe como par\u00e2metro uma lista com os nomes das colunas indesejadas\n)\n\nprint(rm_columns)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "rm_columns = DropColumns(\n    columns=[\"INGLES\"]  # Essa transforma\u00e7\u00e3o recebe como par\u00e2metro uma lista com os nomes das colunas indesejadas\n)\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# Visualizando as colunas do dataset original\nprint(\"Colunas do dataset original: \\n\")\nprint(df.columns)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# Aplicando a transforma\u00e7\u00e3o ``DropColumns`` ao conjunto de dados base\nrm_columns.fit(X=df)\n\n# Reconstruindo um DataFrame Pandas com o resultado da transforma\u00e7\u00e3o\ndf_3 = pd.DataFrame.from_records(\n    data=rm_columns.transform(\n        X=df\n    ),\n)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": "# Visualizando as colunas do dataset transformado\nprint(\"Colunas do dataset ap\u00f3s a transforma\u00e7\u00e3o ``DropColumns``: \\n\")\nprint(df_3.columns)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": false
            },
            "outputs": [],
            "source": "# Visualizando os dados faltantes do dataset ap\u00f3s a primeira transforma\u00e7\u00e3o (df_data_2)\nprint(\"Valores nulos antes da transforma\u00e7\u00e3o SimpleImputer: \\n\\n{}\\n\".format(df_3.isnull().sum(axis = 0)))"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "mediana_col_nota_go = df_3.NOTA_GO.median()\ndf_3 = df_3.fillna({'NOTA_GO': mediana_col_nota_go})\nprint(mediana_col_nota_go)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "df_3.INGLES.value_counts()\ndf_3 = df_3.fillna({'INGLES': 1.0 })"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "print(\"Valores nulos antes da transforma\u00e7\u00e3o SimpleImputer: \\n\\n{}\\n\".format(df_3.isnull().sum(axis = 0)))"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Treinando um modelo de classifica\u00e7\u00e3o"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Finalizado o pr\u00e9-processamento, j\u00e1 temos o conjunto de dados no formato necess\u00e1rio para o treinamento do nosso modelo:"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": "df_3.head(20)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "No exemplo fornecido, iremos utilizar todas as colunas, exceto a coluna **LABELS** como *features* (vari\u00e1veis de entrada).\n\nA vari\u00e1vel **LABELS** ser\u00e1 a vari\u00e1vel-alvo do modelo, conforme descrito no enunciado do desafio."
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "#### Definindo as features do modelo"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# Defini\u00e7\u00e3o das colunas que ser\u00e3o features (nota-se que a coluna NOME n\u00e3o est\u00e1 presente)\nfeatures = [\n    \"MATRICULA\", 'REPROVACOES_DE', 'REPROVACOES_EM', \"REPROVACOES_MF\", \"REPROVACOES_GO\",\n    \"NOTA_DE\", \"NOTA_EM\", \"NOTA_MF\", \"NOTA_GO\",\n    \"INGLES\", \"H_AULA_PRES\", \"TAREFAS_ONLINE\", \"FALTAS\", \n]\n\n# Defini\u00e7\u00e3o da vari\u00e1vel-alvo\ntarget = [\"PERFIL\"]\n\n# Prepara\u00e7\u00e3o dos argumentos para os m\u00e9todos da biblioteca ``scikit-learn``\nX = df_3[features]\ny = df_3[target]"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "O conjunto de entrada (X):"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": "X.head(10)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "As vari\u00e1veis-alvo correspondentes (y):"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "y.head()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "#### Separando o dataset em um conjunto de treino e um conjunto de teste"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Iremos separar o dataset fornecido em dois grupos: um para treinar nosso modelo, e outro para testarmos o resultado atrav\u00e9s de um teste cego. A separa\u00e7\u00e3o do dataset pode ser feita facilmente com o m\u00e9todo *train_test_split()* do scikit-learn:"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# Separa\u00e7\u00e3o dos dados em um conjunto de treino e um conjunto de teste\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=337)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<hr>"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "#### Criando um modelo baseado em \u00e1rvores de decis\u00e3o"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "No exemplo fornecido iremos criar um classificador baseado em **\u00e1rvores de decis\u00e3o**.\n\nMaterial te\u00f3rico sobre \u00e1rvores de decis\u00e3o na documenta\u00e7\u00e3o oficial do scikit-learn: https://scikit-learn.org/stable/modules/tree.html\n\nO primeiro passo \u00e9 basicamente instanciar um objeto *DecisionTreeClassifier()* da biblioteca scikit-learn."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# Cria\u00e7\u00e3o de uma \u00e1rvore de decis\u00e3o com a biblioteca ``scikit-learn``:\ndecision_tree = DecisionTreeClassifier()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "#### Testando o classificador baseado em \u00e1rvore de decis\u00e3o"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# Treino do modelo (\u00e9 chamado o m\u00e9todo *fit()* com os conjuntos de treino)\ndecision_tree.fit(\n    X_train,\n    y_train\n)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "#### Execu\u00e7\u00e3o de predi\u00e7\u00f5es e avalia\u00e7\u00e3o da \u00e1rvore de decis\u00e3o"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# Realiza\u00e7\u00e3o de teste cego no modelo criado\ny_pred = decision_tree.predict(X_test)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "X_test.head()"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "print(y_pred)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "from sklearn.metrics import accuracy_score\n\n# Acur\u00e1cia alcan\u00e7ada pela \u00e1rvore de decis\u00e3o\nprint(\"Acur\u00e1cia: {}%\".format(100*round(accuracy_score(y_test, y_pred), 2)))"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<hr>"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Neste notebook foi demonstrado como trabalhar com transforma\u00e7\u00f5es e modelos com a biblioteca scikit-learn. \u00c9 recomendado que o participante realize seus experimentos editando o c\u00f3digo fornecido aqui at\u00e9 que um modelo com acur\u00e1cia elevada seja alcan\u00e7ado.\n\nQuando voc\u00ea estiver satisfeito com seu modelo, pode passar para a segunda etapa do desafio -- encapsular seu modelo como uma API REST pronta para uso com o Watson Machine Learning!\n\nO notebook para a segunda etapa j\u00e1 se encontra neste projeto, basta acessar a aba **ASSETS** e inicializ\u00e1-lo! N\u00e3o se esqueca de antes desligar o Kernel deste notebook para reduzir o consumo de sua camada gr\u00e1tis do IBM Cloud Pak for Data."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": ""
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": ""
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.6",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.6.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}